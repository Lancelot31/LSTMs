{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import psutil\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\")\n",
    "\n",
    "# Write the path to the repository from dennybritz, we'll use some helper functions\n",
    "if \"../reinforcement-learning/lib\" not in sys.path:\n",
    "  sys.path.append(\"../reinforcement-learning/lib\")\n",
    "\n",
    "import plotting\n",
    "from collections import deque, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build the name of the folder where we'll store the results\n",
    "basename = \"DQN_BlocksWorld\"\n",
    "suffix = datetime.datetime.now().strftime(\"%y%m%d_%H%M%S\")\n",
    "filename = \"_\".join([basename, suffix]) # e.g. 'BlocksWorld_120508_171442'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define the parameters of the problem and the LSTM\n",
    "#Number of blocks in the blocksworld environment\n",
    "numBlocks = 2\n",
    "\n",
    "#Defines the dimension of the input of the LSTM, we use current state and the goal state as the input\n",
    "#For example [0,1] [2,0] (initial state,goal state) has dimension 2*2\n",
    "n_input = numBlocks*2\n",
    "\n",
    "#Dimensions of the output: [block to be moved][destination of the block]\n",
    "#block to be moved has numBlocks length, and destination of the block has \n",
    "#numBlocks+1 because the block can be moved either on top of another block(numBlocks) but\n",
    "#also on top of the table(+1)\n",
    "n_output = numBlocks*(numBlocks+1)\n",
    "\n",
    "#How many possible actions we have in the environment. We will encode the actions as an integer.\n",
    "VALID_ACTIONS = np.array(range(n_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-12 22:43:58,190] Making new env: BlocksWorld-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.envs.make(\"BlocksWorld-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StateProcessor():\n",
    "    \"\"\"\n",
    "    Processes a raw Atari images. Resizes it and converts it to grayscale.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Build the Tensorflow graph\n",
    "        with tf.variable_scope(\"state_processor\"):\n",
    "            #self.input_state = tf.placeholder(tf.float32)   \n",
    "            self.input_state = tf.placeholder(shape=[n_input], dtype=tf.float32)\n",
    "            self.output = self.input_state        \n",
    "            #self.input_state = tf.placeholder(shape=[210, 160, 3], dtype=tf.uint8)\n",
    "            #self.output = tf.image.rgb_to_grayscale(self.input_state)\n",
    "            #self.output = tf.image.crop_to_bounding_box(self.output, 34, 0, 160, 160)\n",
    "            #self.output = tf.image.resize_images(\n",
    "            #    self.output, [84, 84], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "            #self.output = tf.squeeze(self.output)\n",
    "\n",
    "    def process(self, sess, state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sess: A Tensorflow session object\n",
    "            state: A [210, 160, 3] Atari RGB State\n",
    "\n",
    "        Returns:\n",
    "            A processed [84, 84, 1] state representing grayscale values.\n",
    "        \"\"\"\n",
    "        return sess.run(self.output, { self.input_state: state })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Estimator():\n",
    "    \"\"\"Q-Value Estimator neural network.\n",
    "\n",
    "    This network is used for both the Q-Network and the Target Network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scope=\"estimator\", summaries_dir=None):\n",
    "        self.scope = scope\n",
    "        # Writes Tensorboard summaries to disk\n",
    "        self.summary_writer = None\n",
    "        with tf.variable_scope(scope):\n",
    "            # Build the graph\n",
    "            self._build_model()\n",
    "            if summaries_dir:\n",
    "                summary_dir = os.path.join(summaries_dir, \"summaries_{}\".format(scope))\n",
    "                if not os.path.exists(summary_dir):\n",
    "                    os.makedirs(summary_dir)\n",
    "                self.summary_writer = tf.summary.FileWriter(summary_dir)\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Builds the Tensorflow graph.\n",
    "        \"\"\"\n",
    "\n",
    "        # Placeholders for our input\n",
    "        # Our input are 4 RGB frames of shape 160, 160 each\n",
    "        self.X_pl = tf.placeholder(shape=[None,n_input],dtype=tf.float32,name = \"X\")\n",
    "        #self.X_pl = tf.placeholder(shape=[None, 84, 84, 4], dtype=tf.uint8, name=\"X\")\n",
    "        # The TD target value\n",
    "        self.y_pl = tf.placeholder(shape=[None], dtype=tf.float32, name=\"y\")\n",
    "        # Integer id of which action was selected\n",
    "        self.actions_pl = tf.placeholder(shape=[None], dtype=tf.int32, name=\"actions\")\n",
    "\n",
    "        #X = tf.to_float(self.X_pl) / 255.0\n",
    "        batch_size = tf.shape(self.X_pl)[0]\n",
    "\n",
    "        # Three convolutional layers\n",
    "        #conv1 = tf.contrib.layers.conv2d(\n",
    "        #    X, 32, 8, 4, activation_fn=tf.nn.relu)\n",
    "        #conv2 = tf.contrib.layers.conv2d(\n",
    "        #    conv1, 64, 4, 2, activation_fn=tf.nn.relu)\n",
    "        #conv3 = tf.contrib.layers.conv2d(\n",
    "        #    conv2, 64, 3, 1, activation_fn=tf.nn.relu)\n",
    "\n",
    "        # Fully connected layers\n",
    "        #flattened = tf.contrib.layers.flatten(conv3)\n",
    "        fc1 = tf.contrib.layers.fully_connected(self.X_pl, 64)\n",
    "        self.predictions = tf.contrib.layers.fully_connected(fc1, len(VALID_ACTIONS))\n",
    "\n",
    "        # Get the predictions for the chosen actions only\n",
    "        gather_indices = tf.range(batch_size) * tf.shape(self.predictions)[1] + self.actions_pl\n",
    "        self.action_predictions = tf.gather(tf.reshape(self.predictions, [-1]), gather_indices)\n",
    "\n",
    "        # Calcualte the loss\n",
    "        self.losses = tf.squared_difference(self.y_pl, self.action_predictions)\n",
    "        self.loss = tf.reduce_mean(self.losses)\n",
    "\n",
    "        # Optimizer Parameters from original paper\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(0.00025, 0.99, 0.0, 1e-6)\n",
    "        self.train_op = self.optimizer.minimize(self.loss, global_step=tf.contrib.framework.get_global_step())\n",
    "\n",
    "        # Summaries for Tensorboard\n",
    "        self.summaries = tf.summary.merge([\n",
    "            tf.summary.scalar(\"loss\", self.loss),\n",
    "            tf.summary.histogram(\"loss_hist\", self.losses),\n",
    "            tf.summary.histogram(\"q_values_hist\", self.predictions),\n",
    "            tf.summary.scalar(\"max_q_value\", tf.reduce_max(self.predictions))\n",
    "        ])\n",
    "\n",
    "    def predict(self, sess, s):\n",
    "        \"\"\"\n",
    "        Predicts action values.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session\n",
    "          s: State input of shape [batch_size, 4, 160, 160, 3]\n",
    "\n",
    "        Returns:\n",
    "          Tensor of shape [batch_size, NUM_VALID_ACTIONS] containing the estimated \n",
    "          action values.\n",
    "        \"\"\"\n",
    "        return sess.run(self.predictions, { self.X_pl: s })\n",
    "\n",
    "    def update(self, sess, s, a, y):\n",
    "        \"\"\"\n",
    "        Updates the estimator towards the given targets.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session object\n",
    "          s: State input of shape [batch_size, 4, 160, 160, 3]\n",
    "          a: Chosen actions of shape [batch_size]\n",
    "          y: Targets of shape [batch_size]\n",
    "\n",
    "        Returns:\n",
    "          The calculated loss on the batch.\n",
    "        \"\"\"\n",
    "        feed_dict = { self.X_pl: s, self.y_pl: y, self.actions_pl: a }\n",
    "        summaries, global_step, _, loss = sess.run(\n",
    "            [self.summaries, tf.contrib.framework.get_global_step(), self.train_op, self.loss],\n",
    "            feed_dict)\n",
    "        if self.summary_writer:\n",
    "            self.summary_writer.add_summary(summaries, global_step)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rubengarzon/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n",
      "(1, 4)\n",
      "(2, 4)\n",
      "[[ 0.24574825  0.28982329  0.          0.          0.10361734  0.00793611]\n",
      " [ 0.24574825  0.28982329  0.          0.          0.10361734  0.00793611]]\n",
      "97.1438\n"
     ]
    }
   ],
   "source": [
    "# For Testing....\n",
    "\n",
    "tf.reset_default_graph()\n",
    "global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "\n",
    "e = Estimator(scope=\"test\")\n",
    "sp = StateProcessor()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Example observation batch\n",
    "    observation = env.reset()\n",
    "    \n",
    "    observation_p = sp.process(sess, observation)\n",
    "    print (observation_p.shape)\n",
    "    observations = np.reshape(observation_p,[1,n_input]) \n",
    "        \n",
    "    print (observations.shape)\n",
    "    #observation = np.stack([observation_p] * 4, axis=2)\n",
    "    observations = np.array([observation_p] * 2)\n",
    "    print (observations.shape)\n",
    "    # Test Prediction\n",
    "    print(e.predict(sess, observations))\n",
    "\n",
    "    # Test training step\n",
    "    y = np.array([10.0, 10.0])\n",
    "    a = np.array([1, 3])\n",
    "    print(e.update(sess, observations, a, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ModelParametersCopier():\n",
    "    \"\"\"\n",
    "    Copy model parameters of one estimator to another.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, estimator1, estimator2):\n",
    "        \"\"\"\n",
    "        Defines copy-work operation graph.  \n",
    "        Args:\n",
    "          estimator1: Estimator to copy the paramters from\n",
    "          estimator2: Estimator to copy the parameters to\n",
    "        \"\"\"\n",
    "        e1_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator1.scope)]\n",
    "        e1_params = sorted(e1_params, key=lambda v: v.name)\n",
    "        e2_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator2.scope)]\n",
    "        e2_params = sorted(e2_params, key=lambda v: v.name)\n",
    "\n",
    "        self.update_ops = []\n",
    "        for e1_v, e2_v in zip(e1_params, e2_params):\n",
    "            op = e2_v.assign(e1_v)\n",
    "            self.update_ops.append(op)\n",
    "            \n",
    "    def make(self, sess):\n",
    "        \"\"\"\n",
    "        Makes copy.\n",
    "        Args:\n",
    "            sess: Tensorflow session instance\n",
    "        \"\"\"\n",
    "        sess.run(self.update_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(estimator, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon.\n",
    "\n",
    "    Args:\n",
    "        estimator: An estimator that returns q values for a given state\n",
    "        nA: Number of actions in the environment.\n",
    "\n",
    "    Returns:\n",
    "        A function that takes the (sess, observation, epsilon) as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA.\n",
    "\n",
    "    \"\"\"\n",
    "    def policy_fn(sess, observation, epsilon):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        q_values = estimator.predict(sess, np.expand_dims(observation, 0))[0]\n",
    "        best_action = np.argmax(q_values)\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deep_q_learning(sess,\n",
    "                    env,\n",
    "                    q_estimator,\n",
    "                    target_estimator,\n",
    "                    state_processor,\n",
    "                    num_episodes,\n",
    "                    experiment_dir,\n",
    "                    replay_memory_size=500000,\n",
    "                    replay_memory_init_size=50000,\n",
    "                    update_target_estimator_every=10000,\n",
    "                    discount_factor=0.99,\n",
    "                    epsilon_start=1.0,\n",
    "                    epsilon_end=0.1,\n",
    "                    epsilon_decay_steps=500000,\n",
    "                    batch_size=32,\n",
    "                    record_video_every=50):\n",
    "    \"\"\"\n",
    "    Q-Learning algorithm for off-policy TD control using Function Approximation.\n",
    "    Finds the optimal greedy policy while following an epsilon-greedy policy.\n",
    "\n",
    "    Args:\n",
    "        sess: Tensorflow Session object\n",
    "        env: OpenAI environment\n",
    "        q_estimator: Estimator object used for the q values\n",
    "        target_estimator: Estimator object used for the targets\n",
    "        state_processor: A StateProcessor object\n",
    "        num_episodes: Number of episodes to run for\n",
    "        experiment_dir: Directory to save Tensorflow summaries in\n",
    "        replay_memory_size: Size of the replay memory\n",
    "        replay_memory_init_size: Number of random experiences to sampel when initializing \n",
    "          the reply memory.\n",
    "        update_target_estimator_every: Copy parameters from the Q estimator to the \n",
    "          target estimator every N steps\n",
    "        discount_factor: Gamma discount factor\n",
    "        epsilon_start: Chance to sample a random action when taking an action.\n",
    "          Epsilon is decayed over time and this is the start value\n",
    "        epsilon_end: The final minimum value of epsilon after decaying is done\n",
    "        epsilon_decay_steps: Number of steps to decay epsilon over\n",
    "        batch_size: Size of batches to sample from the replay memory\n",
    "        record_video_every: Record a video every N episodes\n",
    "\n",
    "    Returns:\n",
    "        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \"\"\"\n",
    "\n",
    "    Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "\n",
    "    # The replay memory\n",
    "    replay_memory = []\n",
    "    \n",
    "    # Make model copier object\n",
    "    estimator_copy = ModelParametersCopier(q_estimator, target_estimator)\n",
    "\n",
    "    # Keeps track of useful statistics\n",
    "    stats = plotting.EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))\n",
    "    \n",
    "    # For 'system/' summaries, usefull to check if currrent process looks healthy\n",
    "    current_process = psutil.Process()\n",
    "\n",
    "    # Create directories for checkpoints and summaries\n",
    "    checkpoint_dir = os.path.join(experiment_dir, \"checkpoints\")\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, \"model\")\n",
    "    monitor_path = os.path.join(experiment_dir, \"monitor\")\n",
    "    \n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    if not os.path.exists(monitor_path):\n",
    "        os.makedirs(monitor_path)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    # Load a previous checkpoint if we find one\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    if latest_checkpoint:\n",
    "        print(\"Loading model checkpoint {}...\\n\".format(latest_checkpoint))\n",
    "        saver.restore(sess, latest_checkpoint)\n",
    "    \n",
    "    # Get the current time step\n",
    "    total_t = sess.run(tf.contrib.framework.get_global_step())\n",
    "\n",
    "    # The epsilon decay schedule\n",
    "    epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_steps)\n",
    "\n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(\n",
    "        q_estimator,\n",
    "        len(VALID_ACTIONS))\n",
    "\n",
    "    # Populate the replay memory with initial experience\n",
    "    print(\"Populating replay memory...\")\n",
    "    state = env.reset()\n",
    "    state = state_processor.process(sess, state)\n",
    "    #state = np.stack([state] * 4, axis=2)\n",
    "    for i in range(replay_memory_init_size):\n",
    "        action_probs = policy(sess, state, epsilons[min(total_t, epsilon_decay_steps-1)])\n",
    "        action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "        next_state, reward, done, _ = env.step(VALID_ACTIONS[action])\n",
    "        #env.render()\n",
    "        next_state = state_processor.process(sess, next_state)\n",
    "        #next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
    "        replay_memory.append(Transition(state, action, reward, next_state, done))\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            state = state_processor.process(sess, state)\n",
    "            #state = np.stack([state] * 4, axis=2)\n",
    "        else:\n",
    "            state = next_state\n",
    "\n",
    "\n",
    "    # Record videos\n",
    "    # Add env Monitor wrapper\n",
    "    #env = Monitor(env, directory=monitor_path, video_callable=lambda count: count % record_video_every == 0, resume=True)\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "\n",
    "        # Save the current checkpoint\n",
    "        saver.save(tf.get_default_session(), checkpoint_path)\n",
    "\n",
    "        # Reset the environment\n",
    "        state = env.reset()\n",
    "        state = state_processor.process(sess, state)\n",
    "        #state = np.stack([state] * 4, axis=2)\n",
    "        loss = None\n",
    "\n",
    "        # One step in the environment\n",
    "        for t in itertools.count():\n",
    "\n",
    "            # Epsilon for this time step\n",
    "            epsilon = epsilons[min(total_t, epsilon_decay_steps-1)]\n",
    "\n",
    "            # Maybe update the target estimator\n",
    "            if total_t % update_target_estimator_every == 0:\n",
    "                estimator_copy.make(sess)\n",
    "                print(\"\\nCopied model parameters to target network.\")\n",
    "\n",
    "            # Print out which step we're on, useful for debugging.\n",
    "            print(\"\\rStep {} ({}) @ Episode {}/{}, loss: {}\".format(\n",
    "                    t, total_t, i_episode + 1, num_episodes, loss), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            # Take a step\n",
    "            action_probs = policy(sess, state, epsilon)\n",
    "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "            next_state, reward, done, _ = env.step(VALID_ACTIONS[action])\n",
    "            env.render()\n",
    "            next_state = state_processor.process(sess, next_state)\n",
    "            #next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
    "\n",
    "            # If our replay memory is full, pop the first element\n",
    "            if len(replay_memory) == replay_memory_size:\n",
    "                replay_memory.pop(0)\n",
    "\n",
    "            # Save transition to replay memory\n",
    "            replay_memory.append(Transition(state, action, reward, next_state, done))   \n",
    "\n",
    "            # Update statistics\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] = t\n",
    "\n",
    "            # Sample a minibatch from the replay memory\n",
    "            samples = random.sample(replay_memory, batch_size)\n",
    "            states_batch, action_batch, reward_batch, next_states_batch, done_batch = map(np.array, zip(*samples))\n",
    "\n",
    "            # Calculate q values and targets\n",
    "            q_values_next = target_estimator.predict(sess, next_states_batch)\n",
    "            targets_batch = reward_batch + np.invert(done_batch).astype(np.float32) * discount_factor * np.amax(q_values_next, axis=1)\n",
    "\n",
    "            # Perform gradient descent update\n",
    "            states_batch = np.array(states_batch)\n",
    "            loss = q_estimator.update(sess, states_batch, action_batch, targets_batch)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "            total_t += 1\n",
    "\n",
    "        # Add summaries to tensorboard\n",
    "        episode_summary = tf.Summary()\n",
    "        episode_summary.value.add(simple_value=epsilon, tag=\"episode/epsilon\")\n",
    "        episode_summary.value.add(simple_value=stats.episode_rewards[i_episode], tag=\"episode/reward\")\n",
    "        episode_summary.value.add(simple_value=stats.episode_lengths[i_episode], tag=\"episode/length\")\n",
    "        episode_summary.value.add(simple_value=current_process.cpu_percent(), tag=\"system/cpu_usage_percent\")\n",
    "        episode_summary.value.add(simple_value=current_process.memory_percent(memtype=\"vms\"), tag=\"system/v_memeory_usage_percent\")\n",
    "        q_estimator.summary_writer.add_summary(episode_summary, i_episode)\n",
    "        q_estimator.summary_writer.flush()\n",
    "        \n",
    "        yield total_t, plotting.EpisodeStats(\n",
    "            episode_lengths=stats.episode_lengths[:i_episode+1],\n",
    "            episode_rewards=stats.episode_rewards[:i_episode+1])\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rubengarzon/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating replay memory...\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Copied model parameters to target network.\n",
      "Step 0 (0) @ Episode 1/50, loss: None************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 1]\n",
      "Reward: -10\n",
      "Total Episode Reward: -10.0\n",
      "Step 1 (1) @ Episode 1/50, loss: 80.11528778076172************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 1]\n",
      "Reward: -10\n",
      "Total Episode Reward: -20.0\n",
      "Step 2 (2) @ Episode 1/50, loss: 77.07231140136719************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 1]\n",
      "Reward: -10\n",
      "Total Episode Reward: -30.0\n",
      "Step 3 (3) @ Episode 1/50, loss: 69.35395812988281************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 1]\n",
      "Reward: -10\n",
      "Total Episode Reward: -40.0\n",
      "Step 4 (4) @ Episode 1/50, loss: 75.41236114501953************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 1]\n",
      "Reward: -10\n",
      "Total Episode Reward: -50.0\n",
      "Step 5 (5) @ Episode 1/50, loss: 74.53871154785156************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 1]\n",
      "Reward: -10\n",
      "Total Episode Reward: -60.0\n",
      "Step 6 (6) @ Episode 1/50, loss: 68.92304992675781************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 1]\n",
      "Reward: -10\n",
      "Total Episode Reward: -70.0\n",
      "Step 7 (7) @ Episode 1/50, loss: 68.73919677734375************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 1]\n",
      "Reward: -10\n",
      "Total Episode Reward: -80.0\n",
      "Step 8 (8) @ Episode 1/50, loss: 74.49482727050781************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 1]\n",
      "Reward: -10\n",
      "Total Episode Reward: -90.0\n",
      "Step 9 (9) @ Episode 1/50, loss: 62.79640197753906************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 1]\n",
      "Reward: -10\n",
      "Total Episode Reward: -100.0\n",
      "Step 10 (10) @ Episode 1/50, loss: 77.96987915039062************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [0, 0]\n",
      "Goal state:    [0, 1]\n",
      "Reward: -1\n",
      "Total Episode Reward: -101.0\n",
      "Step 11 (11) @ Episode 1/50, loss: 81.03539276123047************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [0, 0]\n",
      "Goal state:    [0, 1]\n",
      "Reward: -10\n",
      "Total Episode Reward: -111.0\n",
      "Step 12 (12) @ Episode 1/50, loss: 71.03202819824219************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 1]\n",
      "Reward: -1\n",
      "Total Episode Reward: -112.0\n",
      "Step 13 (13) @ Episode 1/50, loss: 69.14456176757812************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 1]\n",
      "Reward: -10\n",
      "Total Episode Reward: -122.0\n",
      "Step 14 (14) @ Episode 1/50, loss: 68.85298156738281************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 1]\n",
      "Reward: -10\n",
      "Total Episode Reward: -132.0\n",
      "Step 15 (15) @ Episode 1/50, loss: 74.44667053222656************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 1]\n",
      "Reward: -10\n",
      "Total Episode Reward: -142.0\n",
      "Step 16 (16) @ Episode 1/50, loss: 80.72743225097656************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 1]\n",
      "Reward: -10\n",
      "Total Episode Reward: -152.0\n",
      "Step 17 (17) @ Episode 1/50, loss: 80.36842346191406************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 1]\n",
      "Reward: -10\n",
      "Total Episode Reward: -162.0\n",
      "Step 18 (18) @ Episode 1/50, loss: 62.72076416015625************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 1]\n",
      "Reward: -10\n",
      "Total Episode Reward: -172.0\n",
      "Step 19 (19) @ Episode 1/50, loss: 75.16376495361328************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 1]\n",
      "Reward: -10\n",
      "Total Episode Reward: -182.0\n",
      "Step 20 (20) @ Episode 1/50, loss: 80.72300720214844************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 1]\n",
      "Reward: -10\n",
      "Total Episode Reward: -192.0\n",
      "Step 21 (21) @ Episode 1/50, loss: 79.558349609375************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 1]\n",
      "Reward: -10\n",
      "Total Episode Reward: -202.0\n",
      "Step 22 (22) @ Episode 1/50, loss: 74.82959747314453************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 1]\n",
      "Reward: -10\n",
      "Total Episode Reward: -212.0\n",
      "Step 23 (23) @ Episode 1/50, loss: 64.9830093383789************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 1]\n",
      "Reward: -10\n",
      "Total Episode Reward: -222.0\n",
      "Step 24 (24) @ Episode 1/50, loss: 77.50385284423828************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 1]\n",
      "Reward: -10\n",
      "Total Episode Reward: -232.0\n",
      "Step 25 (25) @ Episode 1/50, loss: 56.95085906982422************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 1]\n",
      "Reward: -10\n",
      "Total Episode Reward: -242.0\n",
      "Step 26 (26) @ Episode 1/50, loss: 89.104248046875************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 1]\n",
      "Reward: -10\n",
      "Total Episode Reward: -252.0\n",
      "Step 27 (27) @ Episode 1/50, loss: 68.10671997070312************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 1]\n",
      "Reward: -10\n",
      "Total Episode Reward: -262.0\n",
      "Step 28 (28) @ Episode 1/50, loss: 74.82759857177734************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 1]\n",
      "Reward: -10\n",
      "Total Episode Reward: -272.0\n",
      "Step 29 (29) @ Episode 1/50, loss: 79.693603515625************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 1]\n",
      "Reward: -10\n",
      "Total Episode Reward: -282.0\n",
      "Step 30 (30) @ Episode 1/50, loss: 68.21102905273438************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 1]\n",
      "Reward: -10\n",
      "Total Episode Reward: -292.0\n",
      "Step 31 (31) @ Episode 1/50, loss: 65.2634506225586************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 1]\n",
      "Reward: -10\n",
      "Total Episode Reward: -302.0\n",
      "Step 32 (32) @ Episode 1/50, loss: 59.70726776123047************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 1]\n",
      "Reward: -10\n",
      "Total Episode Reward: -312.0\n",
      "Step 33 (33) @ Episode 1/50, loss: 88.21353149414062************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 1]\n",
      "Reward: -10\n",
      "Total Episode Reward: -322.0\n",
      "Step 34 (34) @ Episode 1/50, loss: 79.46249389648438************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 1]\n",
      "Reward: -10\n",
      "Total Episode Reward: -332.0\n",
      "Step 35 (35) @ Episode 1/50, loss: 73.99102020263672************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [0, 0]\n",
      "Goal state:    [0, 1]\n",
      "Reward: -1\n",
      "Total Episode Reward: -333.0\n",
      "Step 36 (36) @ Episode 1/50, loss: 79.73004150390625************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [0, 0]\n",
      "Goal state:    [0, 1]\n",
      "Reward: -10\n",
      "Total Episode Reward: -343.0\n",
      "Step 37 (37) @ Episode 1/50, loss: 62.3040885925293\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [0, 1]\n",
      "Reward: 0\n",
      "Total Episode Reward: -343.0\n",
      "\n",
      "Episode Reward: -343.0\n",
      "Step 0 (37) @ Episode 2/50, loss: None************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 0]\n",
      "Current state: [0, 0]\n",
      "Goal state:    [0, 1]\n",
      "Reward: -10\n",
      "Total Episode Reward: -10.0\n",
      "Step 1 (38) @ Episode 2/50, loss: 67.8965072631836\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 0]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [0, 1]\n",
      "Reward: 0\n",
      "Total Episode Reward: -10.0\n",
      "\n",
      "Episode Reward: -10.0\n",
      "Step 0 (38) @ Episode 3/50, loss: None************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -10.0\n",
      "Step 1 (39) @ Episode 3/50, loss: 70.39246368408203************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -20.0\n",
      "Step 2 (40) @ Episode 3/50, loss: 74.3255844116211************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -30.0\n",
      "Step 3 (41) @ Episode 3/50, loss: 73.90711975097656************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -40.0\n",
      "Step 4 (42) @ Episode 3/50, loss: 76.93617248535156************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -50.0\n",
      "Step 5 (43) @ Episode 3/50, loss: 85.47858428955078************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -60.0\n",
      "Step 6 (44) @ Episode 3/50, loss: 68.52359008789062************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -70.0\n",
      "Step 7 (45) @ Episode 3/50, loss: 84.52400970458984************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -80.0\n",
      "Step 8 (46) @ Episode 3/50, loss: 72.39796447753906************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -90.0\n",
      "Step 9 (47) @ Episode 3/50, loss: 70.3114242553711************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -100.0\n",
      "Step 10 (48) @ Episode 3/50, loss: 81.04766082763672************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -110.0\n",
      "Step 11 (49) @ Episode 3/50, loss: 70.6568374633789************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 0]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -1\n",
      "Total Episode Reward: -111.0\n",
      "\n",
      "Copied model parameters to target network.\n",
      "Step 12 (50) @ Episode 3/50, loss: 73.5536117553711\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [2, 0]\n",
      "Reward: 0\n",
      "Total Episode Reward: -111.0\n",
      "\n",
      "Episode Reward: -111.0\n",
      "\n",
      "Copied model parameters to target network.\n",
      "Step 0 (50) @ Episode 4/50, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 0]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [0, 1]\n",
      "Reward: 0\n",
      "Total Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Copied model parameters to target network.\n",
      "Step 0 (50) @ Episode 5/50, loss: None************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -10.0\n",
      "Step 1 (51) @ Episode 5/50, loss: 69.42045593261719************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -20.0\n",
      "Step 2 (52) @ Episode 5/50, loss: 81.4483642578125************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -30.0\n",
      "Step 3 (53) @ Episode 5/50, loss: 76.29505920410156************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -40.0\n",
      "Step 4 (54) @ Episode 5/50, loss: 69.73764038085938************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -50.0\n",
      "Step 5 (55) @ Episode 5/50, loss: 72.97942352294922************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -60.0\n",
      "Step 6 (56) @ Episode 5/50, loss: 63.7593879699707************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -70.0\n",
      "Step 7 (57) @ Episode 5/50, loss: 61.09486389160156************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -80.0\n",
      "Step 8 (58) @ Episode 5/50, loss: 82.27629852294922************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -90.0\n",
      "Step 9 (59) @ Episode 5/50, loss: 75.19282531738281************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -100.0\n",
      "Step 10 (60) @ Episode 5/50, loss: 73.00732421875************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -110.0\n",
      "Step 11 (61) @ Episode 5/50, loss: 72.22484588623047************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -120.0\n",
      "Step 12 (62) @ Episode 5/50, loss: 69.32133483886719************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -130.0\n",
      "Step 13 (63) @ Episode 5/50, loss: 81.46832275390625************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -140.0\n",
      "Step 14 (64) @ Episode 5/50, loss: 84.48579406738281************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -150.0\n",
      "Step 15 (65) @ Episode 5/50, loss: 66.260009765625************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -160.0\n",
      "Step 16 (66) @ Episode 5/50, loss: 63.28321838378906************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -170.0\n",
      "Step 17 (67) @ Episode 5/50, loss: 68.63743591308594************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -180.0\n",
      "Step 18 (68) @ Episode 5/50, loss: 74.59680938720703************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -190.0\n",
      "Step 19 (69) @ Episode 5/50, loss: 72.12150573730469************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -200.0\n",
      "Step 20 (70) @ Episode 5/50, loss: 84.19384002685547************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -210.0\n",
      "Step 21 (71) @ Episode 5/50, loss: 65.73932647705078************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -220.0\n",
      "Step 22 (72) @ Episode 5/50, loss: 72.78598022460938************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -230.0\n",
      "Step 23 (73) @ Episode 5/50, loss: 69.4246826171875************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -240.0\n",
      "Step 24 (74) @ Episode 5/50, loss: 75.90682983398438************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -250.0\n",
      "Step 25 (75) @ Episode 5/50, loss: 72.35865020751953************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -260.0\n",
      "Step 26 (76) @ Episode 5/50, loss: 79.05026245117188************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -270.0\n",
      "Step 27 (77) @ Episode 5/50, loss: 72.7090835571289************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -280.0\n",
      "Step 28 (78) @ Episode 5/50, loss: 83.60416412353516************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -290.0\n",
      "Step 29 (79) @ Episode 5/50, loss: 75.02226257324219************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -300.0\n",
      "Step 30 (80) @ Episode 5/50, loss: 72.28968811035156************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -310.0\n",
      "Step 31 (81) @ Episode 5/50, loss: 78.50650024414062************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -320.0\n",
      "Step 32 (82) @ Episode 5/50, loss: 78.19422149658203************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -330.0\n",
      "Step 33 (83) @ Episode 5/50, loss: 72.03789520263672************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -340.0\n",
      "Step 34 (84) @ Episode 5/50, loss: 69.7088623046875************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -350.0\n",
      "Step 35 (85) @ Episode 5/50, loss: 76.37355041503906************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -360.0\n",
      "Step 36 (86) @ Episode 5/50, loss: 66.26983642578125************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -370.0\n",
      "Step 37 (87) @ Episode 5/50, loss: 75.4699478149414************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -380.0\n",
      "Step 38 (88) @ Episode 5/50, loss: 72.21405029296875************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -390.0\n",
      "Step 39 (89) @ Episode 5/50, loss: 67.34245300292969************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -400.0\n",
      "Step 40 (90) @ Episode 5/50, loss: 86.98812103271484************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -410.0\n",
      "Step 41 (91) @ Episode 5/50, loss: 75.32595825195312************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 0]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -1\n",
      "Total Episode Reward: -411.0\n",
      "Step 42 (92) @ Episode 5/50, loss: 81.30485534667969\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [2, 0]\n",
      "Reward: 0\n",
      "Total Episode Reward: -411.0\n",
      "\n",
      "Episode Reward: -411.0\n",
      "Step 0 (92) @ Episode 6/50, loss: None************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 0]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -1\n",
      "Total Episode Reward: -1.0\n",
      "Step 1 (93) @ Episode 6/50, loss: 78.50004577636719\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [2, 0]\n",
      "Reward: 0\n",
      "Total Episode Reward: -1.0\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 0 (93) @ Episode 7/50, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 0]\n",
      "Goal state:    [0, 0]\n",
      "Reward: 0\n",
      "Total Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (93) @ Episode 8/50, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 0]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [0, 1]\n",
      "Reward: 0\n",
      "Total Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (93) @ Episode 9/50, loss: None************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [0, 0]\n",
      "Goal state:    [0, 1]\n",
      "Reward: -1\n",
      "Total Episode Reward: -1.0\n",
      "Step 1 (94) @ Episode 9/50, loss: 71.91508483886719\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [0, 1]\n",
      "Reward: 0\n",
      "Total Episode Reward: -1.0\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 0 (94) @ Episode 10/50, loss: None************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [0, 0]\n",
      "Goal state:    [0, 1]\n",
      "Reward: -1\n",
      "Total Episode Reward: -1.0\n",
      "Step 1 (95) @ Episode 10/50, loss: 63.25990295410156\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [0, 1]\n",
      "Reward: 0\n",
      "Total Episode Reward: -1.0\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 0 (95) @ Episode 11/50, loss: None************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [0, 0]\n",
      "Goal state:    [0, 1]\n",
      "Reward: -1\n",
      "Total Episode Reward: -1.0\n",
      "Step 1 (96) @ Episode 11/50, loss: 72.24884033203125\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [0, 1]\n",
      "Reward: 0\n",
      "Total Episode Reward: -1.0\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 0 (96) @ Episode 12/50, loss: None************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 0]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -1\n",
      "Total Episode Reward: -1.0\n",
      "Step 1 (97) @ Episode 12/50, loss: 66.44634246826172\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [2, 0]\n",
      "Reward: 0\n",
      "Total Episode Reward: -1.0\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 0 (97) @ Episode 13/50, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [2, 0]\n",
      "Reward: 0\n",
      "Total Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (97) @ Episode 14/50, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 0]\n",
      "Goal state:    [0, 0]\n",
      "Reward: 0\n",
      "Total Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (97) @ Episode 15/50, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 0]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [0, 1]\n",
      "Reward: 0\n",
      "Total Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (97) @ Episode 16/50, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [2, 0]\n",
      "Reward: 0\n",
      "Total Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (97) @ Episode 17/50, loss: None************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 0]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -1\n",
      "Total Episode Reward: -1.0\n",
      "Step 1 (98) @ Episode 17/50, loss: 79.97039031982422\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [2, 0]\n",
      "Reward: 0\n",
      "Total Episode Reward: -1.0\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 0 (98) @ Episode 18/50, loss: None************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -10.0\n",
      "Step 1 (99) @ Episode 18/50, loss: 74.48503112792969************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -20.0\n",
      "\n",
      "Copied model parameters to target network.\n",
      "Step 2 (100) @ Episode 18/50, loss: 75.47679138183594************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -30.0\n",
      "Step 3 (101) @ Episode 18/50, loss: 76.34761810302734************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -40.0\n",
      "Step 4 (102) @ Episode 18/50, loss: 55.17406463623047************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -50.0\n",
      "Step 5 (103) @ Episode 18/50, loss: 76.53693389892578************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -60.0\n",
      "Step 6 (104) @ Episode 18/50, loss: 88.54193115234375************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -70.0\n",
      "Step 7 (105) @ Episode 18/50, loss: 73.8485107421875************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -80.0\n",
      "Step 8 (106) @ Episode 18/50, loss: 70.766845703125************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -90.0\n",
      "Step 9 (107) @ Episode 18/50, loss: 76.65093994140625************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -100.0\n",
      "Step 10 (108) @ Episode 18/50, loss: 70.67041778564453************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -110.0\n",
      "Step 11 (109) @ Episode 18/50, loss: 79.60014343261719************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -120.0\n",
      "Step 12 (110) @ Episode 18/50, loss: 61.50041198730469************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -130.0\n",
      "Step 13 (111) @ Episode 18/50, loss: 70.15967559814453\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [0, 0]\n",
      "Goal state:    [0, 0]\n",
      "Reward: 0\n",
      "Total Episode Reward: -130.0\n",
      "\n",
      "Episode Reward: -130.0\n",
      "Step 0 (111) @ Episode 19/50, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 0]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [0, 1]\n",
      "Reward: 0\n",
      "Total Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (111) @ Episode 20/50, loss: None************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 0]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -1\n",
      "Total Episode Reward: -1.0\n",
      "Step 1 (112) @ Episode 20/50, loss: 64.29563903808594\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [2, 0]\n",
      "Reward: 0\n",
      "Total Episode Reward: -1.0\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 0 (112) @ Episode 21/50, loss: None************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -10.0\n",
      "Step 1 (113) @ Episode 21/50, loss: 82.26683044433594************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -20.0\n",
      "Step 2 (114) @ Episode 21/50, loss: 82.2012710571289************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -30.0\n",
      "Step 3 (115) @ Episode 21/50, loss: 85.32733154296875************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -40.0\n",
      "Step 4 (116) @ Episode 21/50, loss: 70.32202911376953************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -50.0\n",
      "Step 5 (117) @ Episode 21/50, loss: 76.37379455566406************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -60.0\n",
      "Step 6 (118) @ Episode 21/50, loss: 82.827880859375************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -70.0\n",
      "Step 7 (119) @ Episode 21/50, loss: 73.34857940673828************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -80.0\n",
      "Step 8 (120) @ Episode 21/50, loss: 64.40174865722656************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -90.0\n",
      "Step 9 (121) @ Episode 21/50, loss: 79.55846405029297************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -100.0\n",
      "Step 10 (122) @ Episode 21/50, loss: 67.53749084472656************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -110.0\n",
      "Step 11 (123) @ Episode 21/50, loss: 76.43059539794922************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -120.0\n",
      "Step 12 (124) @ Episode 21/50, loss: 76.40959930419922************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -130.0\n",
      "Step 13 (125) @ Episode 21/50, loss: 79.6478271484375************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -140.0\n",
      "Step 14 (126) @ Episode 21/50, loss: 76.39067077636719************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -150.0\n",
      "Step 15 (127) @ Episode 21/50, loss: 70.11479187011719************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -160.0\n",
      "Step 16 (128) @ Episode 21/50, loss: 73.36331176757812************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -170.0\n",
      "Step 17 (129) @ Episode 21/50, loss: 82.38475799560547************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -180.0\n",
      "Step 18 (130) @ Episode 21/50, loss: 72.76657104492188************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -190.0\n",
      "Step 19 (131) @ Episode 21/50, loss: 76.751708984375************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -200.0\n",
      "Step 20 (132) @ Episode 21/50, loss: 67.67123413085938************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -210.0\n",
      "Step 21 (133) @ Episode 21/50, loss: 73.07501983642578************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -220.0\n",
      "Step 22 (134) @ Episode 21/50, loss: 85.54701232910156************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -230.0\n",
      "Step 23 (135) @ Episode 21/50, loss: 64.08128356933594************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -240.0\n",
      "Step 24 (136) @ Episode 21/50, loss: 73.42926025390625************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -250.0\n",
      "Step 25 (137) @ Episode 21/50, loss: 73.2257080078125************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [0, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -260.0\n",
      "Step 26 (138) @ Episode 21/50, loss: 72.98292541503906\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [0, 0]\n",
      "Goal state:    [0, 0]\n",
      "Reward: 0\n",
      "Total Episode Reward: -260.0\n",
      "\n",
      "Episode Reward: -260.0\n",
      "Step 0 (138) @ Episode 22/50, loss: None************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 0]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -1\n",
      "Total Episode Reward: -1.0\n",
      "Step 1 (139) @ Episode 22/50, loss: 75.76032257080078************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 0]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -11.0\n",
      "Step 2 (140) @ Episode 22/50, loss: 79.3826904296875************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 0]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -21.0\n",
      "Step 3 (141) @ Episode 22/50, loss: 79.16545104980469************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 0]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -31.0\n",
      "Step 4 (142) @ Episode 22/50, loss: 67.48611450195312************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 0]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -41.0\n",
      "Step 5 (143) @ Episode 22/50, loss: 88.26881408691406************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 0]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -51.0\n",
      "Step 6 (144) @ Episode 22/50, loss: 85.29728698730469************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 0]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -61.0\n",
      "Step 7 (145) @ Episode 22/50, loss: 79.68682861328125************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 0]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -71.0\n",
      "Step 8 (146) @ Episode 22/50, loss: 67.5765151977539************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 0]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -81.0\n",
      "Step 9 (147) @ Episode 22/50, loss: 66.94856262207031************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 0]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -91.0\n",
      "Step 10 (148) @ Episode 22/50, loss: 76.01075744628906************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 0]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -101.0\n",
      "Step 11 (149) @ Episode 22/50, loss: 85.23419189453125************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 0]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -111.0\n",
      "\n",
      "Copied model parameters to target network.\n",
      "Step 12 (150) @ Episode 22/50, loss: 72.8302993774414************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 0]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -121.0\n",
      "Step 13 (151) @ Episode 22/50, loss: 82.72138977050781************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 0]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -131.0\n",
      "Step 14 (152) @ Episode 22/50, loss: 73.84669494628906************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 0]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -141.0\n",
      "Step 15 (153) @ Episode 22/50, loss: 76.87236022949219\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [2, 0]\n",
      "Reward: 0\n",
      "Total Episode Reward: -141.0\n",
      "\n",
      "Episode Reward: -141.0\n",
      "Step 0 (153) @ Episode 23/50, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 0]\n",
      "Goal state:    [0, 0]\n",
      "Reward: 0\n",
      "Total Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (153) @ Episode 24/50, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 0]\n",
      "Goal state:    [0, 0]\n",
      "Reward: 0\n",
      "Total Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (153) @ Episode 25/50, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 0]\n",
      "Goal state:    [0, 0]\n",
      "Reward: 0\n",
      "Total Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (153) @ Episode 26/50, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [2, 0]\n",
      "Reward: 0\n",
      "Total Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (153) @ Episode 27/50, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 0]\n",
      "Goal state:    [0, 0]\n",
      "Reward: 0\n",
      "Total Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (153) @ Episode 28/50, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [2, 0]\n",
      "Reward: 0\n",
      "Total Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (153) @ Episode 29/50, loss: None************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [0, 0]\n",
      "Goal state:    [0, 1]\n",
      "Reward: -1\n",
      "Total Episode Reward: -1.0\n",
      "Step 1 (154) @ Episode 29/50, loss: 80.08353424072266\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [0, 1]\n",
      "Reward: 0\n",
      "Total Episode Reward: -1.0\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 0 (154) @ Episode 30/50, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [2, 0]\n",
      "Reward: 0\n",
      "Total Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (154) @ Episode 31/50, loss: None************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [0, 0]\n",
      "Goal state:    [0, 1]\n",
      "Reward: -1\n",
      "Total Episode Reward: -1.0\n",
      "Step 1 (155) @ Episode 31/50, loss: 83.16802978515625\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [0, 1]\n",
      "Reward: 0\n",
      "Total Episode Reward: -1.0\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 0 (155) @ Episode 32/50, loss: None************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [0, 0]\n",
      "Goal state:    [0, 1]\n",
      "Reward: -1\n",
      "Total Episode Reward: -1.0\n",
      "Step 1 (156) @ Episode 32/50, loss: 74.00511169433594************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [0, 0]\n",
      "Goal state:    [0, 1]\n",
      "Reward: -10\n",
      "Total Episode Reward: -11.0\n",
      "Step 2 (157) @ Episode 32/50, loss: 73.9932861328125\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [0, 1]\n",
      "Reward: 0\n",
      "Total Episode Reward: -11.0\n",
      "\n",
      "Episode Reward: -11.0\n",
      "Step 0 (157) @ Episode 33/50, loss: None************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [0, 0]\n",
      "Goal state:    [0, 1]\n",
      "Reward: -1\n",
      "Total Episode Reward: -1.0\n",
      "Step 1 (158) @ Episode 33/50, loss: 73.8236312866211\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [0, 1]\n",
      "Reward: 0\n",
      "Total Episode Reward: -1.0\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 0 (158) @ Episode 34/50, loss: None************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 0]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -1\n",
      "Total Episode Reward: -1.0\n",
      "Step 1 (159) @ Episode 34/50, loss: 79.70094299316406************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 0]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -11.0\n",
      "Step 2 (160) @ Episode 34/50, loss: 85.65458679199219\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [2, 0]\n",
      "Reward: 0\n",
      "Total Episode Reward: -11.0\n",
      "\n",
      "Episode Reward: -11.0\n",
      "Step 0 (160) @ Episode 35/50, loss: None************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 0]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -1\n",
      "Total Episode Reward: -1.0\n",
      "Step 1 (161) @ Episode 35/50, loss: 77.03939056396484\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [2, 0]\n",
      "Reward: 0\n",
      "Total Episode Reward: -1.0\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 0 (161) @ Episode 36/50, loss: None************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 0]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -1\n",
      "Total Episode Reward: -1.0\n",
      "Step 1 (162) @ Episode 36/50, loss: 74.24076843261719************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 0]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -11.0\n",
      "Step 2 (163) @ Episode 36/50, loss: 76.72342681884766\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [2, 0]\n",
      "Reward: 0\n",
      "Total Episode Reward: -11.0\n",
      "\n",
      "Episode Reward: -11.0\n",
      "Step 0 (163) @ Episode 37/50, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 0]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [0, 1]\n",
      "Reward: 0\n",
      "Total Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (163) @ Episode 38/50, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 0]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [0, 1]\n",
      "Reward: 0\n",
      "Total Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (163) @ Episode 39/50, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [2, 0]\n",
      "Reward: 0\n",
      "Total Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (163) @ Episode 40/50, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [2, 0]\n",
      "Reward: 0\n",
      "Total Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (163) @ Episode 41/50, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [0, 0]\n",
      "Goal state:    [0, 0]\n",
      "Reward: 0\n",
      "Total Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (163) @ Episode 42/50, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 0]\n",
      "Goal state:    [0, 0]\n",
      "Reward: 0\n",
      "Total Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (163) @ Episode 43/50, loss: None************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [0, 0]\n",
      "Goal state:    [0, 1]\n",
      "Reward: -1\n",
      "Total Episode Reward: -1.0\n",
      "Step 1 (164) @ Episode 43/50, loss: 79.6845703125\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [0, 1]\n",
      "Reward: 0\n",
      "Total Episode Reward: -1.0\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 0 (164) @ Episode 44/50, loss: None************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 0]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -1\n",
      "Total Episode Reward: -1.0\n",
      "Step 1 (165) @ Episode 44/50, loss: 64.6385498046875\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [2, 0]\n",
      "Reward: 0\n",
      "Total Episode Reward: -1.0\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 0 (165) @ Episode 45/50, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [2, 0]\n",
      "Reward: 0\n",
      "Total Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (165) @ Episode 46/50, loss: None************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [0, 0]\n",
      "Goal state:    [0, 1]\n",
      "Reward: -1\n",
      "Total Episode Reward: -1.0\n",
      "Step 1 (166) @ Episode 46/50, loss: 70.53373718261719\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [0, 1]\n",
      "Reward: 0\n",
      "Total Episode Reward: -1.0\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 0 (166) @ Episode 47/50, loss: None************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 0]\n",
      "Current state: [0, 0]\n",
      "Goal state:    [2, 0]\n",
      "Reward: -10\n",
      "Total Episode Reward: -10.0\n",
      "Step 1 (167) @ Episode 47/50, loss: 82.79952239990234\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [2, 0]\n",
      "Reward: 0\n",
      "Total Episode Reward: -10.0\n",
      "\n",
      "Episode Reward: -10.0\n",
      "Step 0 (167) @ Episode 48/50, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 0]\n",
      "Current state: [2, 0]\n",
      "Goal state:    [2, 0]\n",
      "Reward: 0\n",
      "Total Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (167) @ Episode 49/50, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [0, 1]\n",
      "Current state: [0, 0]\n",
      "Goal state:    [0, 0]\n",
      "Reward: 0\n",
      "Total Episode Reward: 0.0\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (167) @ Episode 50/50, loss: None************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [0, 0]\n",
      "Goal state:    [0, 1]\n",
      "Reward: -1\n",
      "Total Episode Reward: -1.0\n",
      "Step 1 (168) @ Episode 50/50, loss: 76.6458969116211\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "************** New Step ***************\n",
      "[block_to_move, destination]; b [0,numBlocks-1], d[0,numBlocks]\n",
      "Initial state: [2, 0]\n",
      "Current state: [0, 1]\n",
      "Goal state:    [0, 1]\n",
      "Reward: 0\n",
      "Total Episode Reward: -1.0\n",
      "\n",
      "Episode Reward: -1.0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Where we save our checkpoints and graphs\n",
    "experiment_dir = os.path.abspath(\"./experiments/{}\".format(filename))\n",
    "#experiment_dir = os.path.abspath(\"./experiments/{}\".format(env.spec.id))\n",
    "\n",
    "# Create a glboal step variable\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    \n",
    "# Create estimators\n",
    "q_estimator = Estimator(scope=\"q_estimator\", summaries_dir=experiment_dir)\n",
    "target_estimator = Estimator(scope=\"target_q\")\n",
    "\n",
    "# State processor\n",
    "state_processor = StateProcessor()\n",
    "\n",
    "# Run it!\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for t, stats in deep_q_learning(sess,\n",
    "                                    env,\n",
    "                                    q_estimator=q_estimator,\n",
    "                                    target_estimator=target_estimator,\n",
    "                                    state_processor=state_processor,\n",
    "                                    experiment_dir=experiment_dir,\n",
    "                                    num_episodes=50,\n",
    "                                    replay_memory_size=8000,\n",
    "                                    replay_memory_init_size=1024,\n",
    "                                    update_target_estimator_every=50,\n",
    "                                    epsilon_start=1.0,\n",
    "                                    epsilon_end=0.1,\n",
    "                                    epsilon_decay_steps=50,\n",
    "                                    discount_factor=0.99,\n",
    "                                    batch_size=32):\n",
    "\n",
    "        print(\"\\nEpisode Reward: {}\".format(stats.episode_rewards[-1]))\n",
    "        \n",
    "        \n",
    "        \n",
    "ep_length,ep_reward,t_steps = plotting.plot_episode_stats (stats, smoothing_window=5,noshow=True)\n",
    "ep_length.savefig(experiment_dir + '/ep_length.png')\n",
    "ep_reward.savefig(experiment_dir + '/ep_reward.png')\n",
    "t_steps.savefig(experiment_dir + '/t_steps.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
