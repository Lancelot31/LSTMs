{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Based on the code from dennybritz\n",
    "#https://github.com/dennybritz/reinforcement-learning\n",
    "#Adapted to the blocksworld environment\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import psutil\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\")\n",
    "\n",
    "# Write the path to the repository from dennybritz, we'll use some helper functions\n",
    "if \"../reinforcement-learning/lib\" not in sys.path:\n",
    "  sys.path.append(\"../reinforcement-learning/lib\")\n",
    "\n",
    "import plotting\n",
    "from collections import deque, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build the name of the folder where we'll store the results\n",
    "basename = \"DQN_BlocksWorld\"\n",
    "suffix = datetime.datetime.now().strftime(\"%y%m%d_%H%M%S\")\n",
    "filename = \"_\".join([basename, suffix]) # e.g. 'BlocksWorld_120508_171442'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define the parameters of the problem and the LSTM\n",
    "#Number of blocks in the blocksworld environment\n",
    "numBlocks = 2\n",
    "\n",
    "#Defines the dimension of the input of the LSTM, we use current state and the goal state as the input\n",
    "#For example [0,1] [2,0] (initial state,goal state) has dimension 2*2\n",
    "n_input = numBlocks*2\n",
    "\n",
    "#Dimensions of the output: [block to be moved][destination of the block]\n",
    "#block to be moved has numBlocks length, and destination of the block has \n",
    "#numBlocks+1 because the block can be moved either on top of another block(numBlocks) but\n",
    "#also on top of the table(+1)\n",
    "n_output = numBlocks*(numBlocks+1)\n",
    "\n",
    "#How many possible actions we have in the environment. We will encode the actions as an integer.\n",
    "VALID_ACTIONS = np.array(range(n_output))\n",
    "\n",
    "#In order to track the convergence of the algorithm we cannot use \n",
    "q_value_dict={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-04-14 22:34:04,403] Making new env: BlocksWorld-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.envs.make(\"BlocksWorld-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StateProcessor():\n",
    "    \"\"\"\n",
    "    Processes a raw Atari images. Resizes it and converts it to grayscale.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Build the Tensorflow graph\n",
    "        with tf.variable_scope(\"state_processor\"):\n",
    "            #self.input_state = tf.placeholder(tf.float32)   \n",
    "            self.input_state = tf.placeholder(shape=[n_input], dtype=tf.float32)\n",
    "            self.output = self.input_state        \n",
    "            #self.input_state = tf.placeholder(shape=[210, 160, 3], dtype=tf.uint8)\n",
    "            #self.output = tf.image.rgb_to_grayscale(self.input_state)\n",
    "            #self.output = tf.image.crop_to_bounding_box(self.output, 34, 0, 160, 160)\n",
    "            #self.output = tf.image.resize_images(\n",
    "            #    self.output, [84, 84], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "            #self.output = tf.squeeze(self.output)\n",
    "\n",
    "    def process(self, sess, state):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sess: A Tensorflow session object\n",
    "            state: A [210, 160, 3] Atari RGB State\n",
    "\n",
    "        Returns:\n",
    "            A processed [84, 84, 1] state representing grayscale values.\n",
    "        \"\"\"\n",
    "        return sess.run(self.output, { self.input_state: state })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Estimator():\n",
    "    \"\"\"Q-Value Estimator neural network.\n",
    "\n",
    "    This network is used for both the Q-Network and the Target Network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, scope=\"estimator\", summaries_dir=None):\n",
    "        self.scope = scope\n",
    "        # Writes Tensorboard summaries to disk\n",
    "        self.summary_writer = None\n",
    "        with tf.variable_scope(scope):\n",
    "            # Build the graph\n",
    "            self._build_model()\n",
    "            if summaries_dir:\n",
    "                summary_dir = os.path.join(summaries_dir, \"summaries_{}\".format(scope))\n",
    "                if not os.path.exists(summary_dir):\n",
    "                    os.makedirs(summary_dir)\n",
    "                self.summary_writer = tf.summary.FileWriter(summary_dir)\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Builds the Tensorflow graph.\n",
    "        \"\"\"\n",
    "        \n",
    "        weights = {\n",
    "            'out': tf.Variable(tf.random_normal([len(VALID_ACTIONS), len(VALID_ACTIONS)]))\n",
    "            }\n",
    "        biases = {\n",
    "            'out': tf.Variable(tf.random_normal([len(VALID_ACTIONS)]))\n",
    "            }\n",
    " \n",
    "        # Placeholders for our input\n",
    "        # Our input are 4 RGB frames of shape 160, 160 each\n",
    "        self.X_pl = tf.placeholder(shape=[None,n_input],dtype=tf.float32,name = \"X\")\n",
    "        #self.X_pl = tf.placeholder(shape=[None, 84, 84, 4], dtype=tf.uint8, name=\"X\")\n",
    "        # The TD target value\n",
    "        self.y_pl = tf.placeholder(shape=[None], dtype=tf.float32, name=\"y\")\n",
    "        # Integer id of which action was selected\n",
    "        self.actions_pl = tf.placeholder(shape=[None], dtype=tf.int32, name=\"actions\")\n",
    "\n",
    "        #X = tf.to_float(self.X_pl) / 255.0\n",
    "        batch_size = tf.shape(self.X_pl)[0]\n",
    "\n",
    "        # Three convolutional layers\n",
    "        #conv1 = tf.contrib.layers.conv2d(\n",
    "        #    X, 32, 8, 4, activation_fn=tf.nn.relu)\n",
    "        #conv2 = tf.contrib.layers.conv2d(\n",
    "        #    conv1, 64, 4, 2, activation_fn=tf.nn.relu)\n",
    "        #conv3 = tf.contrib.layers.conv2d(\n",
    "        #    conv2, 64, 3, 1, activation_fn=tf.nn.relu)\n",
    "\n",
    "        # Fully connected layers\n",
    "        #flattened = tf.contrib.layers.flatten(conv3)\n",
    "        fc1 = tf.contrib.layers.fully_connected(self.X_pl, 64)\n",
    "        last = tf.contrib.layers.fully_connected(fc1, len(VALID_ACTIONS))\n",
    "        \n",
    "        # We need the network to output negative numbers (Rewards are negative or zero, so we add another final linear layer)\n",
    "        self.predictions = tf.matmul(last, weights['out']) + biases['out']\n",
    "        print (self.predictions.shape)\n",
    "\n",
    "        # Get the predictions for the chosen actions only\n",
    "        gather_indices = tf.range(batch_size) * tf.shape(self.predictions)[1] + self.actions_pl\n",
    "        self.action_predictions = tf.gather(tf.reshape(self.predictions, [-1]), gather_indices)\n",
    "\n",
    "        # Calcualte the loss\n",
    "        self.losses = tf.squared_difference(self.y_pl, self.action_predictions)\n",
    "        self.loss = tf.reduce_mean(self.losses)\n",
    "\n",
    "        # Optimizer Parameters from original paper\n",
    "#        self.optimizer = tf.train.RMSPropOptimizer(0.00025, 0.99, 0.0, 1e-6)\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(0.0025, 0.99, 0.0, 1e-6)\n",
    "        self.train_op = self.optimizer.minimize(self.loss, global_step=tf.contrib.framework.get_global_step())\n",
    "\n",
    "        self.max_q_value = tf.reduce_max(self.predictions,1)\n",
    "        print (self.max_q_value.shape)\n",
    "        # Summaries for Tensorboard\n",
    "        self.summaries = tf.summary.merge([\n",
    "            tf.summary.scalar(\"loss\", self.loss),\n",
    "            tf.summary.histogram(\"loss_hist\", self.losses),\n",
    "            tf.summary.histogram(\"q_values_hist\", self.predictions),\n",
    "            tf.summary.scalar(\"max_q_value\", tf.reduce_max(self.predictions))\n",
    "        ])\n",
    "\n",
    "    def predict(self, sess, s):\n",
    "        \"\"\"\n",
    "        Predicts action values.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session\n",
    "          s: State input of shape [batch_size, 4, 160, 160, 3]\n",
    "\n",
    "        Returns:\n",
    "          Tensor of shape [batch_size, NUM_VALID_ACTIONS] containing the estimated \n",
    "          action values.\n",
    "        \"\"\"\n",
    "        return sess.run(self.predictions, { self.X_pl: s })\n",
    "    \n",
    "    def update(self, sess, s, a, y):\n",
    "        \"\"\"\n",
    "        Updates the estimator towards the given targets.\n",
    "\n",
    "        Args:\n",
    "          sess: Tensorflow session object\n",
    "          s: State input of shape [batch_size, 4, 160, 160, 3]\n",
    "          a: Chosen actions of shape [batch_size]\n",
    "          y: Targets of shape [batch_size]\n",
    "\n",
    "        Returns:\n",
    "          The calculated loss on the batch.\n",
    "        \"\"\"\n",
    "        feed_dict = { self.X_pl: s, self.y_pl: y, self.actions_pl: a }\n",
    "        summaries, global_step, _, loss, max_q_value = sess.run(\n",
    "            [self.summaries, tf.contrib.framework.get_global_step(), self.train_op, self.loss,self.max_q_value],\n",
    "            feed_dict)\n",
    "        if self.summary_writer:\n",
    "            self.summary_writer.add_summary(summaries, global_step)        \n",
    "        return loss,max_q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rubengarzon/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?,)\n",
      "[[-1.24555743  0.6225931  -1.0080049  -0.55754375 -0.84093088 -0.89661777]\n",
      " [-1.24555743  0.6225931  -1.0080049  -0.55754375 -0.84093088 -0.89661777]]\n"
     ]
    }
   ],
   "source": [
    "# For Testing....\n",
    "\n",
    "tf.reset_default_graph()\n",
    "global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "\n",
    "e = Estimator(scope=\"test\")\n",
    "sp = StateProcessor()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Example observation batch\n",
    "    observation = env.reset()\n",
    "    \n",
    "    observation_p = sp.process(sess, observation)\n",
    "    #print (observation_p.shape)\n",
    "    observations = np.reshape(observation_p,[1,n_input]) \n",
    "        \n",
    "    #print (observations.shape)\n",
    "    #observation = np.stack([observation_p] * 4, axis=2)\n",
    "    observations = np.array([observation_p] * 2)\n",
    "    #print (observations.shape)\n",
    "    # Test Prediction\n",
    "    print(e.predict(sess, observations))\n",
    "\n",
    "    # Test training step\n",
    "    y = np.array([10.0, 10.0])\n",
    "    a = np.array([1, 3])\n",
    "    #print(e.update(sess, observations, a, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ModelParametersCopier():\n",
    "    \"\"\"\n",
    "    Copy model parameters of one estimator to another.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, estimator1, estimator2):\n",
    "        \"\"\"\n",
    "        Defines copy-work operation graph.  \n",
    "        Args:\n",
    "          estimator1: Estimator to copy the paramters from\n",
    "          estimator2: Estimator to copy the parameters to\n",
    "        \"\"\"\n",
    "        e1_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator1.scope)]\n",
    "        e1_params = sorted(e1_params, key=lambda v: v.name)\n",
    "        e2_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator2.scope)]\n",
    "        e2_params = sorted(e2_params, key=lambda v: v.name)\n",
    "\n",
    "        self.update_ops = []\n",
    "        for e1_v, e2_v in zip(e1_params, e2_params):\n",
    "            op = e2_v.assign(e1_v)\n",
    "            self.update_ops.append(op)\n",
    "            \n",
    "    def make(self, sess):\n",
    "        \"\"\"\n",
    "        Makes copy.\n",
    "        Args:\n",
    "            sess: Tensorflow session instance\n",
    "        \"\"\"\n",
    "        sess.run(self.update_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(estimator, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function approximator and epsilon.\n",
    "\n",
    "    Args:\n",
    "        estimator: An estimator that returns q values for a given state\n",
    "        nA: Number of actions in the environment.\n",
    "\n",
    "    Returns:\n",
    "        A function that takes the (sess, observation, epsilon) as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA.\n",
    "\n",
    "    \"\"\"\n",
    "    def policy_fn(sess, observation, epsilon):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        q_values = estimator.predict(sess, np.expand_dims(observation, 0))[0]\n",
    "        best_action = np.argmax(q_values)\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_q_value (state,q_value):\n",
    "    # state must be a string representing the state, q_value must be a float\n",
    "    if (state not in q_value_dict):\n",
    "        q_value_dict[state]=[]\n",
    "    q_value_dict[state].append(q_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def deep_q_learning(sess,\n",
    "                    env,\n",
    "                    q_estimator,\n",
    "                    target_estimator,\n",
    "                    state_processor,\n",
    "                    num_episodes,\n",
    "                    experiment_dir,\n",
    "                    replay_memory_size=500000,\n",
    "                    replay_memory_init_size=50000,\n",
    "                    update_target_estimator_every=10000,\n",
    "                    discount_factor=0.99,\n",
    "                    epsilon_start=1.0,\n",
    "                    epsilon_end=0.1,\n",
    "                    epsilon_decay_steps=500000,\n",
    "                    batch_size=32,\n",
    "                    record_video_every=50):\n",
    "    \"\"\"\n",
    "    Q-Learning algorithm for off-policy TD control using Function Approximation.\n",
    "    Finds the optimal greedy policy while following an epsilon-greedy policy.\n",
    "\n",
    "    Args:\n",
    "        sess: Tensorflow Session object\n",
    "        env: OpenAI environment\n",
    "        q_estimator: Estimator object used for the q values\n",
    "        target_estimator: Estimator object used for the targets\n",
    "        state_processor: A StateProcessor object\n",
    "        num_episodes: Number of episodes to run for\n",
    "        experiment_dir: Directory to save Tensorflow summaries in\n",
    "        replay_memory_size: Size of the replay memory\n",
    "        replay_memory_init_size: Number of random experiences to sampel when initializing \n",
    "          the reply memory.\n",
    "        update_target_estimator_every: Copy parameters from the Q estimator to the \n",
    "          target estimator every N steps\n",
    "        discount_factor: Gamma discount factor\n",
    "        epsilon_start: Chance to sample a random action when taking an action.\n",
    "          Epsilon is decayed over time and this is the start value\n",
    "        epsilon_end: The final minimum value of epsilon after decaying is done\n",
    "        epsilon_decay_steps: Number of steps to decay epsilon over\n",
    "        batch_size: Size of batches to sample from the replay memory\n",
    "        record_video_every: Record a video every N episodes\n",
    "\n",
    "    Returns:\n",
    "        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \"\"\"\n",
    "\n",
    "    Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "    \n",
    "    #Q values dictionary, one key per state\n",
    "    \n",
    "\n",
    "    # The replay memory\n",
    "    replay_memory = []\n",
    "    \n",
    "    # Make model copier object\n",
    "    estimator_copy = ModelParametersCopier(q_estimator, target_estimator)\n",
    "\n",
    "    # Keeps track of useful statistics\n",
    "    stats = plotting.EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))\n",
    "    \n",
    "    # For 'system/' summaries, usefull to check if currrent process looks healthy\n",
    "    current_process = psutil.Process()\n",
    "\n",
    "    # Create directories for checkpoints and summaries\n",
    "    checkpoint_dir = os.path.join(experiment_dir, \"checkpoints\")\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, \"model\")\n",
    "    monitor_path = os.path.join(experiment_dir, \"monitor\")\n",
    "    \n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    if not os.path.exists(monitor_path):\n",
    "        os.makedirs(monitor_path)\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    # Load a previous checkpoint if we find one\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    if latest_checkpoint:\n",
    "        print(\"Loading model checkpoint {}...\\n\".format(latest_checkpoint))\n",
    "        saver.restore(sess, latest_checkpoint)\n",
    "    \n",
    "    # Get the current time step\n",
    "    total_t = sess.run(tf.contrib.framework.get_global_step())\n",
    "\n",
    "    # The epsilon decay schedule\n",
    "    epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_steps)\n",
    "\n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(\n",
    "        q_estimator,\n",
    "        len(VALID_ACTIONS))\n",
    "\n",
    "    # Populate the replay memory with initial experience\n",
    "    print(\"Populating replay memory...\")\n",
    "    state = env.reset()\n",
    "    state = state_processor.process(sess, state)\n",
    "    #state = np.stack([state] * 4, axis=2)\n",
    "    for i in range(replay_memory_init_size):\n",
    "        action_probs = policy(sess, state, epsilons[min(total_t, epsilon_decay_steps-1)])\n",
    "        action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "        next_state, reward, done, _ = env.step(VALID_ACTIONS[action])\n",
    "        #env.render()\n",
    "        next_state = state_processor.process(sess, next_state)\n",
    "        #next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
    "        replay_memory.append(Transition(state, action, reward, next_state, done))\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            state = state_processor.process(sess, state)\n",
    "            #state = np.stack([state] * 4, axis=2)\n",
    "        else:\n",
    "            state = next_state\n",
    "\n",
    "\n",
    "    # Record videos\n",
    "    # Add env Monitor wrapper\n",
    "    #env = Monitor(env, directory=monitor_path, video_callable=lambda count: count % record_video_every == 0, resume=True)\n",
    "    qvalue_summary = tf.Summary()\n",
    "    for i_episode in range(num_episodes):\n",
    "\n",
    "        # Save the current checkpoint\n",
    "        saver.save(tf.get_default_session(), checkpoint_path)\n",
    "\n",
    "        # Reset the environment\n",
    "        state = env.reset()\n",
    "        state = state_processor.process(sess, state)\n",
    "        #state = np.stack([state] * 4, axis=2)\n",
    "        loss = None\n",
    "\n",
    "        # One step in the environment\n",
    "        for t in itertools.count():\n",
    "\n",
    "            # Epsilon for this time step\n",
    "            epsilon = epsilons[min(total_t, epsilon_decay_steps-1)]\n",
    "\n",
    "            # Maybe update the target estimator\n",
    "            if total_t % update_target_estimator_every == 0:\n",
    "                estimator_copy.make(sess)\n",
    "                print(\"\\nCopied model parameters to target network.\")\n",
    "\n",
    "            # Print out which step we're on, useful for debugging.\n",
    "            print(\"\\rStep {} ({}) @ Episode {}/{}, loss: {}\".format(\n",
    "                    t, total_t, i_episode + 1, num_episodes, loss), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            # Take a step\n",
    "            action_probs = policy(sess, state, epsilon)\n",
    "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "            next_state, reward, done, _ = env.step(VALID_ACTIONS[action])\n",
    "            #env.render()\n",
    "            next_state = state_processor.process(sess, next_state)\n",
    "            #next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
    "\n",
    "            # If our replay memory is full, pop the first element\n",
    "            if len(replay_memory) == replay_memory_size:\n",
    "                replay_memory.pop(0)\n",
    "\n",
    "            # Save transition to replay memory\n",
    "            replay_memory.append(Transition(state, action, reward, next_state, done))   \n",
    "\n",
    "            # Update statistics\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] = t\n",
    "\n",
    "            # Sample a minibatch from the replay memory\n",
    "            samples = random.sample(replay_memory, batch_size)\n",
    "            states_batch, action_batch, reward_batch, next_states_batch, done_batch = map(np.array, zip(*samples))\n",
    "\n",
    "            # Calculate q values and targets\n",
    "            q_values_next = target_estimator.predict(sess, next_states_batch)\n",
    "           \n",
    "            targets_batch = reward_batch + np.invert(done_batch).astype(np.float32) * discount_factor * np.amax(q_values_next, axis=1)\n",
    "\n",
    "            # Perform gradient descent update\n",
    "            states_batch = np.array(states_batch)\n",
    "            #print ('Targets batch')\n",
    "            #print (targets_batch)\n",
    "            loss,max_q_values = q_estimator.update(sess, states_batch, action_batch, targets_batch)\n",
    "            n = 0\n",
    "            for q in max_q_values: \n",
    "                #print (states_batch[n])\n",
    "                #print (q)\n",
    "                s_str = np.array2string(states_batch[n])\n",
    "                add_q_value(s_str,q)\n",
    "                qvalue_summary.value.add(simple_value=np.mean(q_value_dict[s_str]),tag=\"avg_max_q_value \" + s_str)\n",
    "                n = n+1\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "            total_t += 1\n",
    "\n",
    "        # Add summaries to tensorboard\n",
    "        episode_summary = tf.Summary()\n",
    "        episode_summary.value.add(simple_value=epsilon, tag=\"episode/epsilon\")\n",
    "        episode_summary.value.add(simple_value=stats.episode_rewards[i_episode], tag=\"episode/reward\")\n",
    "        episode_summary.value.add(simple_value=stats.episode_lengths[i_episode], tag=\"episode/length\")\n",
    "        episode_summary.value.add(simple_value=current_process.cpu_percent(), tag=\"system/cpu_usage_percent\")\n",
    "        episode_summary.value.add(simple_value=current_process.memory_percent(memtype=\"vms\"), tag=\"system/v_memeory_usage_percent\")\n",
    "        q_estimator.summary_writer.add_summary(episode_summary, i_episode)\n",
    "        q_estimator.summary_writer.add_summary(qvalue_summary,i_episode)\n",
    "        q_estimator.summary_writer.flush()\n",
    "        \n",
    "        yield total_t, plotting.EpisodeStats(\n",
    "            episode_lengths=stats.episode_lengths[:i_episode+1],\n",
    "            episode_rewards=stats.episode_rewards[:i_episode+1])\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rubengarzon/anaconda/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/ops/gradients_impl.py:91: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?,)\n",
      "(?, 6)\n",
      "(?,)\n",
      "Populating replay memory...\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Copied model parameters to target network.\n",
      "Step 56 (56) @ Episode 1/400, loss: 23.362808227539062\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -461.0\n",
      "Step 10 (66) @ Episode 2/400, loss: 22.118738174438477\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -82.0\n",
      "Step 0 (66) @ Episode 3/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (66) @ Episode 4/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 43 (109) @ Episode 5/400, loss: 16.169752120971685\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -421.0\n",
      "Step 6 (115) @ Episode 6/400, loss: 14.971965789794922\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -51.0\n",
      "Step 2 (117) @ Episode 7/400, loss: 18.004081726074229\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -20.0\n",
      "Step 10 (127) @ Episode 8/400, loss: 12.669000625610352\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -91.0\n",
      "Step 6 (133) @ Episode 9/400, loss: 13.208904266357422\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -60.0\n",
      "Step 0 (133) @ Episode 10/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 19 (152) @ Episode 11/400, loss: 11.005472183227539\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -163.0\n",
      "Step 1 (153) @ Episode 12/400, loss: 9.888559341430664\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -10.0\n",
      "Step 5 (158) @ Episode 13/400, loss: 7.8081521987915048\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -50.0\n",
      "Step 2 (160) @ Episode 14/400, loss: 11.71005630493164\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -11.0\n",
      "Step 22 (182) @ Episode 15/400, loss: 12.947557449340827\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -220.0\n",
      "Step 3 (185) @ Episode 16/400, loss: 9.7608346939086917\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -30.0\n",
      "Step 14 (199) @ Episode 17/400, loss: 8.2662944793701173\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -95.0\n",
      "Step 0 (199) @ Episode 18/400, loss: None\n",
      "Copied model parameters to target network.\n",
      "Step 3 (202) @ Episode 18/400, loss: 32.53480529785156\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -30.0\n",
      "Step 3 (205) @ Episode 19/400, loss: 18.068279266357422\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -30.0\n",
      "Step 2 (207) @ Episode 20/400, loss: 20.646436691284182\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -20.0\n",
      "Step 1 (208) @ Episode 21/400, loss: 14.790281295776367\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -10.0\n",
      "Step 4 (212) @ Episode 22/400, loss: 15.118762016296387\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -31.0\n",
      "Step 4 (216) @ Episode 23/400, loss: 16.670604705810547\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -31.0\n",
      "Step 0 (216) @ Episode 24/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (216) @ Episode 25/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (216) @ Episode 26/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 2 (218) @ Episode 27/400, loss: 16.135082244873047\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -20.0\n",
      "Step 0 (218) @ Episode 28/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 1 (219) @ Episode 29/400, loss: 10.034964561462402\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -10.0\n",
      "Step 0 (219) @ Episode 30/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 3 (222) @ Episode 31/400, loss: 12.089180946350098\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -30.0\n",
      "Step 8 (230) @ Episode 32/400, loss: 14.791130065917969\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -71.0\n",
      "Step 14 (244) @ Episode 33/400, loss: 14.353693962097168\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -122.0\n",
      "Step 2 (246) @ Episode 34/400, loss: 11.701092720031738\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -20.0\n",
      "Step 0 (246) @ Episode 35/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 4 (250) @ Episode 36/400, loss: 16.885826110839844\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -22.0\n",
      "Step 0 (250) @ Episode 37/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 5 (255) @ Episode 38/400, loss: 8.3325290679931644\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -32.0\n",
      "Step 7 (262) @ Episode 39/400, loss: 14.744628906255478\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -70.0\n",
      "Step 1 (263) @ Episode 40/400, loss: 10.104612350463867\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -10.0\n",
      "Step 0 (263) @ Episode 41/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 23 (286) @ Episode 42/400, loss: 8.8982715606689458\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -140.0\n",
      "Step 0 (286) @ Episode 43/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 2 (288) @ Episode 44/400, loss: 9.693632125854492\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -11.0\n",
      "Step 0 (288) @ Episode 45/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 6 (294) @ Episode 46/400, loss: 10.246907234191895\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -51.0\n",
      "Step 47 (341) @ Episode 47/400, loss: 10.159889221191406\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -317.0\n",
      "Step 1 (342) @ Episode 48/400, loss: 8.736825942993164\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -10.0\n",
      "Step 0 (342) @ Episode 49/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (342) @ Episode 50/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 12 (354) @ Episode 51/400, loss: 8.212758064270022\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -57.0\n",
      "Step 9 (363) @ Episode 52/400, loss: 14.451042175292969\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -45.0\n",
      "Step 3 (366) @ Episode 53/400, loss: 4.0858459472656254\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -21.0\n",
      "Step 1 (367) @ Episode 54/400, loss: 5.7481536865234375\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -10.0\n",
      "Step 15 (382) @ Episode 55/400, loss: 7.8992109298706055\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -96.0\n",
      "Step 2 (384) @ Episode 56/400, loss: 7.197507858276367\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -20.0\n",
      "Step 0 (384) @ Episode 57/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 6 (390) @ Episode 58/400, loss: 6.264207363128662\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -42.0\n",
      "Step 9 (399) @ Episode 59/400, loss: 3.7878479957580566\n",
      "Copied model parameters to target network.\n",
      "Step 34 (424) @ Episode 59/400, loss: 6.2141571044921875\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -223.0\n",
      "Step 0 (424) @ Episode 60/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 4 (428) @ Episode 61/400, loss: 10.33074951171875\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -31.0\n",
      "Step 3 (431) @ Episode 62/400, loss: 6.8538236618041995\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -30.0\n",
      "Step 0 (431) @ Episode 63/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 1 (432) @ Episode 64/400, loss: 5.142727851867676\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -10.0\n",
      "Step 35 (467) @ Episode 65/400, loss: 5.2114748954772955\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -215.0\n",
      "Step 13 (480) @ Episode 66/400, loss: 3.4905171394348145\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -85.0\n",
      "Step 0 (480) @ Episode 67/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (480) @ Episode 68/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 1 (481) @ Episode 69/400, loss: 6.44075345993042\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 0 (481) @ Episode 70/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 2 (483) @ Episode 71/400, loss: 6.1833615303039558\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -20.0\n",
      "Step 2 (485) @ Episode 72/400, loss: 3.463951587677002\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -20.0\n",
      "Step 13 (498) @ Episode 73/400, loss: 7.5759258270263673\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -40.0\n",
      "Step 35 (533) @ Episode 74/400, loss: 4.2115435600280767\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -152.0\n",
      "Step 4 (537) @ Episode 75/400, loss: 4.712287425994873\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -13.0\n",
      "Step 6 (543) @ Episode 76/400, loss: 3.6892449855804443\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -33.0\n",
      "Step 1 (544) @ Episode 77/400, loss: 3.303515911102295\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -10.0\n",
      "Step 0 (544) @ Episode 78/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 17 (561) @ Episode 79/400, loss: 5.7458295822143555\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -107.0\n",
      "Step 38 (599) @ Episode 80/400, loss: 8.1863079071044925\n",
      "Copied model parameters to target network.\n",
      "Step 97 (658) @ Episode 80/400, loss: 3.0516734123229984\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -394.0\n",
      "Step 0 (658) @ Episode 81/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 5 (663) @ Episode 82/400, loss: 1.7013595104217536\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -41.0\n",
      "Step 0 (663) @ Episode 83/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 3 (666) @ Episode 84/400, loss: 4.7054848670959471\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -3.0\n",
      "Step 1 (667) @ Episode 85/400, loss: 3.160357713699341\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 1 (668) @ Episode 86/400, loss: 3.2513813972473145\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 3 (671) @ Episode 87/400, loss: 1.0095419883728027\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -12.0\n",
      "Step 3 (674) @ Episode 88/400, loss: 2.0234477519989014\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -21.0\n",
      "Step 5 (679) @ Episode 89/400, loss: 2.1070535182952888\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -32.0\n",
      "Step 0 (679) @ Episode 90/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (679) @ Episode 91/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 7 (686) @ Episode 92/400, loss: 1.2175860404968262\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -43.0\n",
      "Step 0 (686) @ Episode 93/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 1 (687) @ Episode 94/400, loss: 3.3450188636779785\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 0 (687) @ Episode 95/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 1 (688) @ Episode 96/400, loss: 0.981648325920105\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -10.0\n",
      "Step 0 (688) @ Episode 97/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 1 (689) @ Episode 98/400, loss: 5.25955867767334\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 0 (689) @ Episode 99/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 1 (690) @ Episode 100/400, loss: 5.915729999542236\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 0 (690) @ Episode 101/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (690) @ Episode 102/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (690) @ Episode 103/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (690) @ Episode 104/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 1 (691) @ Episode 105/400, loss: 4.931192874908447\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -10.0\n",
      "Step 3 (694) @ Episode 106/400, loss: 2.5756788253784183\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -12.0\n",
      "Step 1 (695) @ Episode 107/400, loss: 2.2600433826446533\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -10.0\n",
      "Step 0 (695) @ Episode 108/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 1 (696) @ Episode 109/400, loss: 2.135315418243408\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 0 (696) @ Episode 110/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 1 (697) @ Episode 111/400, loss: 1.2891086339950562\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 2 (699) @ Episode 112/400, loss: 2.1983671188354496\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -20.0\n",
      "Step 1 (700) @ Episode 113/400, loss: 3.493462085723877\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -10.0\n",
      "Step 0 (700) @ Episode 114/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 8 (708) @ Episode 115/400, loss: 2.5828013420104989\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -35.0\n",
      "Step 1 (709) @ Episode 116/400, loss: 2.5387046337127686\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -10.0\n",
      "Step 1 (710) @ Episode 117/400, loss: 5.623895645141602\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 0 (710) @ Episode 118/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 1 (711) @ Episode 119/400, loss: 3.5550646781921387\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 0 (711) @ Episode 120/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (711) @ Episode 121/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 1 (712) @ Episode 122/400, loss: 3.5252573490142822\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -10.0\n",
      "Step 1 (713) @ Episode 123/400, loss: 7.350034236907959\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 0 (713) @ Episode 124/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (713) @ Episode 125/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (713) @ Episode 126/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (713) @ Episode 127/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (713) @ Episode 128/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (713) @ Episode 129/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 3 (716) @ Episode 130/400, loss: 4.0499815940856934\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -21.0\n",
      "Step 2 (718) @ Episode 131/400, loss: 0.24348881840705872\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -20.0\n",
      "Step 1 (719) @ Episode 132/400, loss: 1.6769254207611084\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 1 (720) @ Episode 133/400, loss: 2.3732712268829346\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 1 (721) @ Episode 134/400, loss: 2.8550662994384766\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -10.0\n",
      "Step 0 (721) @ Episode 135/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (721) @ Episode 136/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 3 (724) @ Episode 137/400, loss: 1.9475231170654297\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -21.0\n",
      "Step 1 (725) @ Episode 138/400, loss: 1.4994535446166992\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 0 (725) @ Episode 139/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 5 (730) @ Episode 140/400, loss: 2.2299060821533203\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -32.0\n",
      "Step 26 (756) @ Episode 141/400, loss: 2.3672437667846685\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -80.0\n",
      "Step 0 (756) @ Episode 142/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 2 (758) @ Episode 143/400, loss: 1.0981545448303223\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -2.0\n",
      "Step 2 (760) @ Episode 144/400, loss: 2.6493587493896484\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -11.0\n",
      "Step 0 (760) @ Episode 145/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (760) @ Episode 146/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (760) @ Episode 147/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (760) @ Episode 148/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (760) @ Episode 149/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (760) @ Episode 150/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 1 (761) @ Episode 151/400, loss: 1.9447340965270996\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -10.0\n",
      "Step 1 (762) @ Episode 152/400, loss: 1.025904893875122\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 2 (764) @ Episode 153/400, loss: 1.5660723447799683\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -11.0\n",
      "Step 0 (764) @ Episode 154/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 1 (765) @ Episode 155/400, loss: 2.3720314502716064\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -10.0\n",
      "Step 1 (766) @ Episode 156/400, loss: 4.743563652038574\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 1 (767) @ Episode 157/400, loss: 1.8968863487243652\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 0 (767) @ Episode 158/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (767) @ Episode 159/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (767) @ Episode 160/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 1 (768) @ Episode 161/400, loss: 0.6805838346481323\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 0 (768) @ Episode 162/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (768) @ Episode 163/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (768) @ Episode 164/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (768) @ Episode 165/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 2 (770) @ Episode 166/400, loss: 1.1906883716583252\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -11.0\n",
      "Step 0 (770) @ Episode 167/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (770) @ Episode 168/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 2 (772) @ Episode 169/400, loss: 1.6821141242980957\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -11.0\n",
      "Step 2 (774) @ Episode 170/400, loss: 1.009604573249817\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -11.0\n",
      "Step 1 (775) @ Episode 171/400, loss: 1.6397695541381836\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -10.0\n",
      "Step 2 (777) @ Episode 172/400, loss: 1.1042429208755493\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -11.0\n",
      "Step 3 (780) @ Episode 173/400, loss: 2.5264503955841064\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -21.0\n",
      "Step 1 (781) @ Episode 174/400, loss: 2.7263855934143066\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -10.0\n",
      "Step 1 (782) @ Episode 175/400, loss: 0.8312655687332153\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 2 (784) @ Episode 176/400, loss: 3.2137393951416016\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -11.0\n",
      "Step 0 (784) @ Episode 177/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 1 (785) @ Episode 178/400, loss: 1.1669857501983643\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 0 (785) @ Episode 179/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (785) @ Episode 180/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 1 (786) @ Episode 181/400, loss: 1.8967093229293823\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 0 (786) @ Episode 182/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 1 (787) @ Episode 183/400, loss: 1.1394026279449463\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 0 (787) @ Episode 184/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 1 (788) @ Episode 185/400, loss: 2.5547194480895996\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -10.0\n",
      "Step 0 (788) @ Episode 186/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (788) @ Episode 187/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (788) @ Episode 188/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 2 (790) @ Episode 189/400, loss: 2.3799824714660645\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -20.0\n",
      "Step 0 (790) @ Episode 190/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 2 (792) @ Episode 191/400, loss: 2.1983606815338135\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -11.0\n",
      "Step 0 (792) @ Episode 192/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 1 (793) @ Episode 193/400, loss: 2.068667411804199\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 2 (795) @ Episode 194/400, loss: 1.8491988182067878\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -11.0\n",
      "Step 0 (795) @ Episode 195/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 2 (797) @ Episode 196/400, loss: 1.3094472885131836\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -20.0\n",
      "Step 0 (797) @ Episode 197/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (797) @ Episode 198/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 2 (799) @ Episode 199/400, loss: 1.3001421689987183\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -11.0\n",
      "Step 0 (799) @ Episode 200/400, loss: None\n",
      "Copied model parameters to target network.\n",
      "Step 1 (800) @ Episode 200/400, loss: 1.081491470336914\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -10.0\n",
      "\n",
      "Copied model parameters to target network.\n",
      "Step 0 (800) @ Episode 201/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "\n",
      "Copied model parameters to target network.\n",
      "Step 3 (803) @ Episode 202/400, loss: 0.9504145979881287\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -30.0\n",
      "Step 2 (805) @ Episode 203/400, loss: 1.0433790683746338\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -11.0\n",
      "Step 1 (806) @ Episode 204/400, loss: 2.7793140411376953\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -10.0\n",
      "Step 2 (808) @ Episode 205/400, loss: 1.2328944206237793\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -11.0\n",
      "Step 1 (809) @ Episode 206/400, loss: 1.0112828016281128\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 0 (809) @ Episode 207/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (809) @ Episode 208/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (809) @ Episode 209/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (809) @ Episode 210/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (809) @ Episode 211/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (809) @ Episode 212/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 2 (811) @ Episode 213/400, loss: 1.32789397239685063\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -20.0\n",
      "Step 2 (813) @ Episode 214/400, loss: 2.2825613021850586\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -11.0\n",
      "Step 1 (814) @ Episode 215/400, loss: 0.42351898550987244\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 0 (814) @ Episode 216/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (814) @ Episode 217/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (814) @ Episode 218/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 4 (818) @ Episode 219/400, loss: 1.83562171459198373\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -31.0\n",
      "Step 2 (820) @ Episode 220/400, loss: 0.34320178627967834\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -11.0\n",
      "Step 0 (820) @ Episode 221/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (820) @ Episode 222/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (820) @ Episode 223/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (820) @ Episode 224/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (820) @ Episode 225/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (820) @ Episode 226/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (820) @ Episode 227/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 1 (821) @ Episode 228/400, loss: 1.2887091636657715\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 0 (821) @ Episode 229/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 1 (822) @ Episode 230/400, loss: 0.7505671977996826\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 1 (823) @ Episode 231/400, loss: 0.4552602767944336\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 1 (824) @ Episode 232/400, loss: 0.43224605917930603\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 2 (826) @ Episode 233/400, loss: 0.7444022893905644\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -11.0\n",
      "Step 2 (828) @ Episode 234/400, loss: 0.6611434817314148\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -20.0\n",
      "Step 1 (829) @ Episode 235/400, loss: 0.42114779353141785\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 0 (829) @ Episode 236/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (829) @ Episode 237/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 1 (830) @ Episode 238/400, loss: 1.5361813306808472\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 0 (830) @ Episode 239/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 1 (831) @ Episode 240/400, loss: 0.7086887359619141\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 1 (832) @ Episode 241/400, loss: 0.35816454887390137\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 0 (832) @ Episode 242/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (832) @ Episode 243/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (832) @ Episode 244/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (832) @ Episode 245/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 1 (833) @ Episode 246/400, loss: 0.456564724445343\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -10.0\n",
      "Step 1 (834) @ Episode 247/400, loss: 0.8719289302825928\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -10.0\n",
      "Step 1 (835) @ Episode 248/400, loss: 0.36562591791152954\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -10.0\n",
      "Step 2 (837) @ Episode 249/400, loss: 0.9941837191581726\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -11.0\n",
      "Step 0 (837) @ Episode 250/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 1 (838) @ Episode 251/400, loss: 0.7472448945045471\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -10.0\n",
      "Step 0 (838) @ Episode 252/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 2 (840) @ Episode 253/400, loss: 0.7488327622413635\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -11.0\n",
      "Step 0 (840) @ Episode 254/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 2 (842) @ Episode 255/400, loss: 1.4401781558990479\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -11.0\n",
      "Step 1 (843) @ Episode 256/400, loss: 1.057208776473999\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -10.0\n",
      "Step 0 (843) @ Episode 257/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (843) @ Episode 258/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 2 (845) @ Episode 259/400, loss: 0.5879364013671875\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -11.0\n",
      "Step 0 (845) @ Episode 260/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (845) @ Episode 261/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 1 (846) @ Episode 262/400, loss: 0.6440644860267639\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 0 (846) @ Episode 263/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 2 (848) @ Episode 264/400, loss: 0.7734525799751282\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -11.0\n",
      "Step 0 (848) @ Episode 265/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (848) @ Episode 266/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (848) @ Episode 267/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (848) @ Episode 268/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 1 (849) @ Episode 269/400, loss: 0.80058354139328\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -10.0\n",
      "Step 0 (849) @ Episode 270/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (849) @ Episode 271/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (849) @ Episode 272/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 1 (850) @ Episode 273/400, loss: 0.4601200819015503\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 0 (850) @ Episode 274/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 2 (852) @ Episode 275/400, loss: 0.4154919385910034\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -11.0\n",
      "Step 2 (854) @ Episode 276/400, loss: 0.51941508054733284\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -11.0\n",
      "Step 1 (855) @ Episode 277/400, loss: 1.1630208492279053\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 0 (855) @ Episode 278/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 2 (857) @ Episode 279/400, loss: 0.705194354057312\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -11.0\n",
      "Step 0 (857) @ Episode 280/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (857) @ Episode 281/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 1 (858) @ Episode 282/400, loss: 0.21894778311252594\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -10.0\n",
      "Step 0 (858) @ Episode 283/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (858) @ Episode 284/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (858) @ Episode 285/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (858) @ Episode 286/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 1 (859) @ Episode 287/400, loss: 1.4094992876052856\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 0 (859) @ Episode 288/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 1 (860) @ Episode 289/400, loss: 0.3300849199295044\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 0 (860) @ Episode 290/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 1 (861) @ Episode 291/400, loss: 0.630455493927002\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 0 (861) @ Episode 292/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 1 (862) @ Episode 293/400, loss: 0.6092123985290527\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 0 (862) @ Episode 294/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 2 (864) @ Episode 295/400, loss: 0.3640769124031067\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -11.0\n",
      "Step 1 (865) @ Episode 296/400, loss: 0.20507730543613434\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 1 (866) @ Episode 297/400, loss: 0.3137030005455017\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 5 (871) @ Episode 298/400, loss: 1.10252881050109866\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -23.0\n",
      "Step 3 (874) @ Episode 299/400, loss: 0.32518941164016724\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -21.0\n",
      "Step 0 (874) @ Episode 300/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 1 (875) @ Episode 301/400, loss: 0.8503419756889343\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -10.0\n",
      "Step 2 (877) @ Episode 302/400, loss: 0.3415583372116089\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -11.0\n",
      "Step 3 (880) @ Episode 303/400, loss: 0.8008377552032471\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -12.0\n",
      "Step 0 (880) @ Episode 304/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 2 (882) @ Episode 305/400, loss: 0.2782764136791229\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -2.0\n",
      "Step 0 (882) @ Episode 306/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (882) @ Episode 307/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 2 (884) @ Episode 308/400, loss: 1.0460708141326904\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -11.0\n",
      "Step 1 (885) @ Episode 309/400, loss: 0.5493748188018799\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -10.0\n",
      "Step 3 (888) @ Episode 310/400, loss: 0.30823156237602234\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -21.0\n",
      "Step 1 (889) @ Episode 311/400, loss: 0.9607634544372559\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 1 (890) @ Episode 312/400, loss: 0.9633579850196838\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -10.0\n",
      "Step 2 (892) @ Episode 313/400, loss: 0.24754780530929565\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -20.0\n",
      "Step 1 (893) @ Episode 314/400, loss: 0.5830196142196655\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 1 (894) @ Episode 315/400, loss: 0.5152057409286499\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 0 (894) @ Episode 316/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 1 (895) @ Episode 317/400, loss: 0.4879325032234192\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 2 (897) @ Episode 318/400, loss: 0.06968159973621368\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -11.0\n",
      "Step 0 (897) @ Episode 319/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 1 (898) @ Episode 320/400, loss: 1.441239833831787\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -10.0\n",
      "Step 0 (898) @ Episode 321/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (898) @ Episode 322/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 1 (899) @ Episode 323/400, loss: 0.6034315824508667\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 1 (900) @ Episode 324/400, loss: 0.8966620564460754\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 0 (900) @ Episode 325/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 2 (902) @ Episode 326/400, loss: 0.36917281150817877\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -11.0\n",
      "Step 2 (904) @ Episode 327/400, loss: 0.14960701763629913\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -20.0\n",
      "Step 1 (905) @ Episode 328/400, loss: 0.4358941912651062\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -10.0\n",
      "Step 2 (907) @ Episode 329/400, loss: 0.44820559024810795\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -11.0\n",
      "Step 0 (907) @ Episode 330/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (907) @ Episode 331/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (907) @ Episode 332/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (907) @ Episode 333/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (907) @ Episode 334/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 2 (909) @ Episode 335/400, loss: 0.3002576529979706\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -2.0\n",
      "Step 0 (909) @ Episode 336/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 1 (910) @ Episode 337/400, loss: 2.2798266410827637\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 0 (910) @ Episode 338/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (910) @ Episode 339/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 1 (911) @ Episode 340/400, loss: 0.35412466526031494\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 0 (911) @ Episode 341/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 3 (914) @ Episode 342/400, loss: 0.52065986394882222\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -21.0\n",
      "Step 0 (914) @ Episode 343/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (914) @ Episode 344/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 1 (915) @ Episode 345/400, loss: 1.147005319595337\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 0 (915) @ Episode 346/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 1 (916) @ Episode 347/400, loss: 0.3669533133506775\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 1 (917) @ Episode 348/400, loss: 0.9624801874160767\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 1 (918) @ Episode 349/400, loss: 0.15328380465507507\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 0 (918) @ Episode 350/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 3 (921) @ Episode 351/400, loss: 0.14664089679718018\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -3.0\n",
      "Step 0 (921) @ Episode 352/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (921) @ Episode 353/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 2 (923) @ Episode 354/400, loss: 0.12433106452226639\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -2.0\n",
      "Step 0 (923) @ Episode 355/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 1 (924) @ Episode 356/400, loss: 1.2082228660583496\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 0 (924) @ Episode 357/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (924) @ Episode 358/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 1 (925) @ Episode 359/400, loss: 1.3861578702926636\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -10.0\n",
      "Step 0 (925) @ Episode 360/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (925) @ Episode 361/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (925) @ Episode 362/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (925) @ Episode 363/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (925) @ Episode 364/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (925) @ Episode 365/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 1 (926) @ Episode 366/400, loss: 0.5657697319984436\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 1 (927) @ Episode 367/400, loss: 1.1010127067565918\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 0 (927) @ Episode 368/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 3 (930) @ Episode 369/400, loss: 0.35855621099472046\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -21.0\n",
      "Step 2 (932) @ Episode 370/400, loss: 0.34893918037414556\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -11.0\n",
      "Step 0 (932) @ Episode 371/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (932) @ Episode 372/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (932) @ Episode 373/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 1 (933) @ Episode 374/400, loss: 0.11550185084342957\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -10.0\n",
      "Step 0 (933) @ Episode 375/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (933) @ Episode 376/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (933) @ Episode 377/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (933) @ Episode 378/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (933) @ Episode 379/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (933) @ Episode 380/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 3 (936) @ Episode 381/400, loss: 0.82658588886260994\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -21.0\n",
      "Step 1 (937) @ Episode 382/400, loss: 0.30740877985954285\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 1 (938) @ Episode 383/400, loss: 0.360401451587677\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 0 (938) @ Episode 384/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (938) @ Episode 385/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 1 (939) @ Episode 386/400, loss: 0.2239699512720108\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -10.0\n",
      "Step 0 (939) @ Episode 387/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 1 (940) @ Episode 388/400, loss: 1.0550709962844849\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -10.0\n",
      "Step 1 (941) @ Episode 389/400, loss: 0.6893651485443115\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 1 (942) @ Episode 390/400, loss: 0.24272023141384125\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 0 (942) @ Episode 391/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 1 (943) @ Episode 392/400, loss: 0.6303120851516724\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 1 (944) @ Episode 393/400, loss: 1.1343222856521606\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -1.0\n",
      "Step 2 (946) @ Episode 394/400, loss: 0.9181087017059326\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -11.0\n",
      "Step 0 (946) @ Episode 395/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 2 (948) @ Episode 396/400, loss: 0.42020815610885628\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -11.0\n",
      "Step 2 (950) @ Episode 397/400, loss: 0.11104564368724823\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: -20.0\n",
      "Step 0 (950) @ Episode 398/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (950) @ Episode 399/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n",
      "Step 0 (950) @ Episode 400/400, loss: None\n",
      "*************** PROBLEM SOLVED!!!!!!!!!!!! **********\n",
      "\n",
      "Episode Reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Where we save our checkpoints and graphs\n",
    "experiment_dir = os.path.abspath(\"./experiments/{}\".format(filename))\n",
    "#experiment_dir = os.path.abspath(\"./experiments/{}\".format(env.spec.id))\n",
    "\n",
    "# Create a glboal step variable\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    \n",
    "# Create estimators\n",
    "q_estimator = Estimator(scope=\"q_estimator\", summaries_dir=experiment_dir)\n",
    "target_estimator = Estimator(scope=\"target_q\")\n",
    "\n",
    "# State processor\n",
    "state_processor = StateProcessor()\n",
    "\n",
    "# Run it!\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for t, stats in deep_q_learning(sess,\n",
    "                                    env,\n",
    "                                    q_estimator=q_estimator,\n",
    "                                    target_estimator=target_estimator,\n",
    "                                    state_processor=state_processor,\n",
    "                                    experiment_dir=experiment_dir,\n",
    "                                    num_episodes=400,\n",
    "                                    replay_memory_size=256,\n",
    "                                    replay_memory_init_size=128,\n",
    "                                    update_target_estimator_every=200,\n",
    "                                    epsilon_start=1.0,\n",
    "                                    epsilon_end=0.1,\n",
    "                                    epsilon_decay_steps=1000,\n",
    "                                    discount_factor=0.99,\n",
    "                                    batch_size=32):\n",
    "\n",
    "        print(\"\\nEpisode Reward: {}\".format(stats.episode_rewards[-1]))\n",
    "        \n",
    "        \n",
    "        \n",
    "ep_length,ep_reward,t_steps = plotting.plot_episode_stats (stats, smoothing_window=5,noshow=True)\n",
    "ep_length.savefig(experiment_dir + '/ep_length.png')\n",
    "ep_reward.savefig(experiment_dir + '/ep_reward.png')\n",
    "t_steps.savefig(experiment_dir + '/t_steps.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
